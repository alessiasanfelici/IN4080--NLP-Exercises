{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IN4080 - 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mandatory Assignment 2** - Alessia Sanfelici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1\t– Exploring\tthe NLTK\ttagger landscape**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1a:\tData\tSplit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first experiments, we limit ourselves to the news section of the Brown corpus and split it into a \n",
    "training (90%) and a validation (10%) set. (We don’t need a test set for the moment, but will build one in \n",
    "part 3.) Moreover, we use the universal tagset instead of the default one.\n",
    "\n",
    "You should store your data split in the variables news_train and news_val. Note that you may need \n",
    "to download the corpus first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\sanfe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = brown.tagged_sents(categories = 'news', tagset = 'universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train, news_val = train_test_split(sents, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1b:\tMost\tFrequent\tClass\tBaseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of part-of-speech tags is typically quite skewed, with the most frequent class in general \n",
    "being common nouns. As a simple baseline, we should thus know how a model that always predicts the \n",
    "same (most frequent) class performs. This can be done with nltk.DefaultTagger. Note that we are using the universal tagset, so the \n",
    "most frequent tag is not named NN. Evaluate it on the validation set and report the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent tag is called NOUN\n"
     ]
    }
   ],
   "source": [
    "tags = [tag for (word, tag) in sents[0]]\n",
    "max_tag = nltk.FreqDist(tags).max()\n",
    "print(\"The most frequent tag is called\", max_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.2996151189183855\n"
     ]
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "default_accuracy = default_tagger.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(default_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1c:\tNaïve\tBayes\tUnigram\tTagger**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first models discussed in course is a Naïve Bayes classifier that relies only on the current word \n",
    "and does not take any context into account. This model is available as nltk.UnigramTagger. Report the accuracy on the \n",
    "validation set. How does the accuracy on the universal tagset differ from the one reported on the default \n",
    "tagset in the NLTK book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.8853251751702359\n"
     ]
    }
   ],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(news_train)\n",
    "unigram_accuracy = unigram_tagger.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(unigram_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy in this case results be be lower than the one reported on the default tagset in the NLTK book. It is lower of around 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1d:\tBigram\tHMM\tTagger**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lectures, we spent quite some time on the HMM tagger. Evaluate it on the validation set and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9121681634264285\n"
     ]
    }
   ],
   "source": [
    "hmm = nltk.HiddenMarkovModelTagger.train(news_train)\n",
    "hmm_accuracy = hmm.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(hmm_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1e:\tPerceptron\twith\tgreedy\tdecoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lectures, we have shortly discussed Matthew Honnibal’s proposal of a structured perceptron\n",
    "tagger with greedy decoding. He argued that an extended set of features is more helpful for tagging than \n",
    "exact (Viterbi) decoding. Evaluate it on the validation set and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9654593901115168\n"
     ]
    }
   ],
   "source": [
    "perc = nltk.PerceptronTagger(load = False)\n",
    "perc.train(news_train)\n",
    "perc_accuracy = perc.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(perc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the results of the previous exercises and discuss them in a few sentences. Do the accuracies \n",
    "correspond to your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Default Tagger</th>\n",
       "      <td>0.299615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigram Tagger</th>\n",
       "      <td>0.885325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Markov Model Tagger</th>\n",
       "      <td>0.912168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron Tagger</th>\n",
       "      <td>0.965459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy\n",
       "Default Tagger              0.299615\n",
       "Unigram Tagger              0.885325\n",
       "Hidden Markov Model Tagger  0.912168\n",
       "Perceptron Tagger           0.965459"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = [default_accuracy, unigram_accuracy, hmm_accuracy, perc_accuracy]\n",
    "taggers = [\"Default Tagger\", \"Unigram Tagger\", \"Hidden Markov Model Tagger\", \"Perceptron Tagger\"]\n",
    "\n",
    "df_accuracies = pd.DataFrame(accuracies, columns = [\"Accuracy\"], index = taggers)\n",
    "df_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... discussion ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part\t2\t– Greedy\tLR\ttaggers\tand\tfeature\tengineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2a:\tGetting\tstarted\twith\ta\tgreedy\tlogistic\tregression tagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScikitGreedyTagger(nltk.TaggerI):\n",
    "    def __init__(self, features, clf=LogisticRegression(max_iter = 400)):\n",
    "        self.features = features\n",
    "        self.classifier = clf\n",
    "        self.vectorizer = DictVectorizer()\n",
    "\n",
    "    def train(self, train_sents):\n",
    "        train_feature_sets = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                feature_set = self.features(untagged_sent, i, history)\n",
    "                train_feature_sets.append(feature_set)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        x_train = self.vectorizer.fit_transform(train_feature_sets)\n",
    "        y_train = np.array(train_labels)\n",
    "        self.classifier.fit(x_train, y_train)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.vectorizer.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.924010658245337\n"
     ]
    }
   ],
   "source": [
    "lr_tagger = ScikitGreedyTagger(pos_features)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the accuracy of this tagger compare to the taggers tested in part 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".........."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2b:\tAdding\tword\tcontext\tfeatures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic feature function contains the previous and the current word. Also add the next word and the \n",
    "word before the previous one. Describe which combination works best and keep it for the next \n",
    "experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9243067206158098\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of previous word, current word and the word before the previous one\n",
    "\n",
    "def pos_features_1(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "        features[\"2_prev_word\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"2_prev_word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"2_prev_word\"] = sentence[i-2]\n",
    "    return features\n",
    "\n",
    "lr_tagger = ScikitGreedyTagger(pos_features_1)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.934372841211882\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of previous word, current word and next word\n",
    "\n",
    "def pos_features_2(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "    try:\n",
    "        features[\"next_word\"] = sentence[i+1]\n",
    "    except:\n",
    "        features[\"next_word\"] = \"<END>\"\n",
    "    return features\n",
    "\n",
    "lr_tagger = ScikitGreedyTagger(pos_features_2)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.933978091384585\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of previous word, current word, next word and the word before the previous one\n",
    "\n",
    "def pos_features_both(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "        features[\"2_prev_word\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"2_prev_word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"2_prev_word\"] = sentence[i-2]\n",
    "    try:\n",
    "        features[\"next_word\"] = sentence[i+1]\n",
    "    except:\n",
    "        features[\"next_word\"] = \"<END>\"\n",
    "    return features\n",
    "\n",
    "lr_tagger = ScikitGreedyTagger(pos_features_both)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination containing the previous word, the current word and the next word works a bit better than the other combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2c:\tAdding\ttransition\tfeatures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the feature function to include the tag predicted at the previous position. Does this help? What \n",
    "about a trigram model that includes the two previously predicted tags?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2d:\tEven\tmore\tfeatures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to add more features to get an even better tagger. Only the fantasy sets limits to what you may \n",
    "consider. Some ideas: Extract suffixes and prefixes from the current, previous or next word. Is the \n",
    "current word a number? Is it capitalized? Does it contain capitals? Does it contain a hyphen? etc. What is \n",
    "the best feature set you can come up with? Train and test various feature sets and select the best one.\n",
    "If you use sources for finding tips about good features (like articles, web pages, NLTK code, etc.) make \n",
    "references to the sources and explain what you got from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2e:\tRegularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment, we will study the effect of different regularization strengths now. In scikit\u0002learn, regularization is expressed by the parameter C. A smaller C means stronger regularization. Try with \n",
    "C in [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] and see which value which yields the best result. You can also try \n",
    "additional values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize your experiments to make clear which set of features and parameters provide the best \n",
    "results, and what the corresponding accuracy score is. Did you manage to outperform the perceptron \n",
    "tagger? If not, where do you think the bottleneck of your current tagger lies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part\t3\t– Training\tand\ttesting\ton\ta\tlarger\tcorpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus covers 15 different genres, but we have only explored the news genre so far. In this \n",
    "part, we will retrain the most promising taggers on an extended set of genres and test them on held-out \n",
    "data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t3a:\tCompile\tthe\textended\ttraining\tand\ttest\tdata**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK book, chapter 2.1.3, lists the names of the 15 genres available in the Brown corpus. We will set \n",
    "two genres aside for testing: hobbies and adventure. For training, we will use the news training set \n",
    "prepared for the previous exercises, as well as the data from the remaining 12 genres. Prepare the \n",
    "corpus as described and store the datasets in the variables all_train, hobbies_test and \n",
    "adventure_test. We will not use news_val in this part. Make sure to use the universal tagset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t3b:\tEvaluate\tthe\ttaggers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the most successful tagger from part 1 and the best setup from part 2. Retrain both of them on \n",
    "all_train and evaluate them separately on the two test genres. Report the results and discuss them \n",
    "briefly: Which of the two genres is “easier”? How well do the two taggers generalize to unseen genres?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t3c:\tConfusion\tmatrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy gives us a high-level overview of the performance of a tagger, but we may be interested in \n",
    "finding out more details about where the tagger makes the mistakes. The universal tagset is reasonably \n",
    "small, so we can produce a confusion matrix. Take a look at https://www.nltk.org/api/nltk.tag.api.html\n",
    "and make a confusion matrix for the results. Pick the results of one test set and one tagger. Make sure \n",
    "you understand what the rows and columns are. Which pairs of tags are most easily confounded?\n",
    "You can find the documentation of the tagset in the following link, but note that NLTK uses an earlier, \n",
    "slightly different version of the tagset: https://universaldependencies.org/u/pos/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t3d:\tPrecision,\trecall\tand\tf-measure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding hints on the NLTK web page linked above, calculate the precision, recall and f-measure for each \n",
    "tag and display the results in a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also calculate the macro precision, macro recall and macro f-measure across all tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t3e:\tError\tanalysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it makes sense to inspect the output of a machine learning model more thoroughly. Find five \n",
    "sentences in the test set where at least one token is misclassified and display these sentences in the \n",
    "following format, with both the predicted and gold tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the words that are tagged differently. Comment on each of the differences. Would you say that \n",
    "the predicted tag is wrong? Or is there a genuine ambiguity such that both answers are defendable? Or is \n",
    "even the gold tag wrong?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
