{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IN4080 - 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mandatory Assignment 2** - Alessia Sanfelici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1\t– Exploring\tthe NLTK\ttagger landscape**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1a:\tData\tSplit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first experiments, we limit ourselves to the news section of the Brown corpus and split it into a \n",
    "training (90%) and a validation (10%) set. (We don’t need a test set for the moment, but will build one in \n",
    "part 3.) Moreover, we use the universal tagset instead of the default one.\n",
    "\n",
    "You should store your data split in the variables news_train and news_val. Note that you may need \n",
    "to download the corpus first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\sanfe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = brown.tagged_sents(categories = 'news', tagset = 'universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train, news_val = train_test_split(sents, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1b:\tMost\tFrequent\tClass\tBaseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of part-of-speech tags is typically quite skewed, with the most frequent class in general \n",
    "being common nouns. As a simple baseline, we should thus know how a model that always predicts the \n",
    "same (most frequent) class performs. This can be done with nltk.DefaultTagger. Note that we are using the universal tagset, so the \n",
    "most frequent tag is not named NN. Evaluate it on the validation set and report the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent tag is called NOUN\n"
     ]
    }
   ],
   "source": [
    "tags = [tag for (word, tag) in sents[0]]\n",
    "max_tag = nltk.FreqDist(tags).max()\n",
    "print(\"The most frequent tag is called\", max_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.2996151189183855\n"
     ]
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "default_accuracy = default_tagger.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(default_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1c:\tNaïve\tBayes\tUnigram\tTagger**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first models discussed in course is a Naïve Bayes classifier that relies only on the current word \n",
    "and does not take any context into account. This model is available as nltk.UnigramTagger. Report the accuracy on the \n",
    "validation set. How does the accuracy on the universal tagset differ from the one reported on the default \n",
    "tagset in the NLTK book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.8853251751702359\n"
     ]
    }
   ],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(news_train)\n",
    "unigram_accuracy = unigram_tagger.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(unigram_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy in this case results to be lower than the one reported on the default tagset in the NLTK book. It is lower of around 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1d:\tBigram\tHMM\tTagger**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lectures, we spent quite some time on the HMM tagger. Evaluate it on the validation set and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9121681634264285\n"
     ]
    }
   ],
   "source": [
    "hmm = nltk.HiddenMarkovModelTagger.train(news_train)\n",
    "hmm_accuracy = hmm.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(hmm_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1e:\tPerceptron\twith\tgreedy\tdecoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lectures, we have shortly discussed Matthew Honnibal’s proposal of a structured perceptron\n",
    "tagger with greedy decoding. He argued that an extended set of features is more helpful for tagging than \n",
    "exact (Viterbi) decoding. Evaluate it on the validation set and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9650646402842199\n"
     ]
    }
   ],
   "source": [
    "perc = nltk.PerceptronTagger(load = False)\n",
    "perc.train(news_train)\n",
    "perc_accuracy = perc.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(perc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the results of the previous exercises and discuss them in a few sentences. Do the accuracies \n",
    "correspond to your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Default Tagger</th>\n",
       "      <td>0.299615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigram Tagger</th>\n",
       "      <td>0.885325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Markov Model Tagger</th>\n",
       "      <td>0.912168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron Tagger</th>\n",
       "      <td>0.965065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy\n",
       "Default Tagger              0.299615\n",
       "Unigram Tagger              0.885325\n",
       "Hidden Markov Model Tagger  0.912168\n",
       "Perceptron Tagger           0.965065"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = [default_accuracy, unigram_accuracy, hmm_accuracy, perc_accuracy]\n",
    "taggers = [\"Default Tagger\", \"Unigram Tagger\", \"Hidden Markov Model Tagger\", \"Perceptron Tagger\"]\n",
    "\n",
    "df_accuracies = pd.DataFrame(accuracies, columns = [\"Accuracy\"], index = taggers)\n",
    "df_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained results are shown in the previous dataframe. The Defaul Tagger results to be the worst choice in this case, with an accuracy that is very small with respect to the other methods. For Unigram Tagger, HMM tagger and Perceptron tagger, the results ar similar, but the best one is the Percepton tagger. Its accuracy is around 96.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... discussion ..... what expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part\t2\t– Greedy\tLR\ttaggers\tand\tfeature\tengineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2a:\tGetting\tstarted\twith\ta\tgreedy\tlogistic\tregression tagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScikitGreedyTagger(nltk.TaggerI):\n",
    "    def __init__(self, features, clf=LogisticRegression(max_iter = 400)):\n",
    "        self.features = features\n",
    "        self.classifier = clf\n",
    "        self.vectorizer = DictVectorizer()\n",
    "\n",
    "    def train(self, train_sents):\n",
    "        train_feature_sets = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                feature_set = self.features(untagged_sent, i, history)\n",
    "                train_feature_sets.append(feature_set)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        x_train = self.vectorizer.fit_transform(train_feature_sets)\n",
    "        y_train = np.array(train_labels)\n",
    "        self.classifier.fit(x_train, y_train)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.vectorizer.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.924010658245337\n"
     ]
    }
   ],
   "source": [
    "lr_tagger = ScikitGreedyTagger(pos_features)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the accuracy of this tagger compare to the taggers tested in part 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this tagger results to be good with respect to the results of the previous part: the tagger reaches the second position for highest accuray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2b:\tAdding\tword\tcontext\tfeatures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic feature function contains the previous and the current word. Also add the next word and the \n",
    "word before the previous one. Describe which combination works best and keep it for the next \n",
    "experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9243067206158098\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of previous word, current word and the word before the previous one\n",
    "\n",
    "def pos_features_1(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "        features[\"2_prev_word\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"2_prev_word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"2_prev_word\"] = sentence[i-2]\n",
    "    return features\n",
    "\n",
    "lr_tagger = ScikitGreedyTagger(pos_features_1)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.934372841211882\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of previous word, current word and next word\n",
    "\n",
    "def pos_features_2(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "    try:\n",
    "        features[\"next_word\"] = sentence[i+1]\n",
    "    except:\n",
    "        features[\"next_word\"] = \"<END>\"\n",
    "    return features\n",
    "\n",
    "lr_tagger = ScikitGreedyTagger(pos_features_2)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.933978091384585\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of previous word, current word, next word and the word before the previous one\n",
    "\n",
    "def pos_features_both(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "        features[\"2_prev_word\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"2_prev_word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"2_prev_word\"] = sentence[i-2]\n",
    "    try:\n",
    "        features[\"next_word\"] = sentence[i+1]\n",
    "    except:\n",
    "        features[\"next_word\"] = \"<END>\"\n",
    "    return features\n",
    "\n",
    "lr_tagger = ScikitGreedyTagger(pos_features_both)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination containing the previous word, the current word and the next word works a bit better than the other combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2c:\tAdding\ttransition\tfeatures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the feature function to include the tag predicted at the previous position. Does this help? What \n",
    "about a trigram model that includes the two previously predicted tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanfe\\OneDrive\\Desktop\\NLP\\NLP\\NLP-Projects\\alesssa_in4080_mandatory_2.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m lr_tagger\u001b[39m.\u001b[39mtrain(news_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(lr_tagger\u001b[39m.\u001b[39;49maccuracy(news_val))\n",
      "File \u001b[1;32mc:\\Users\\sanfe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tag\\api.py:74\u001b[0m, in \u001b[0;36mTaggerI.accuracy\u001b[1;34m(self, gold)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccuracy\u001b[39m(\u001b[39mself\u001b[39m, gold):\n\u001b[0;32m     64\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m    Score the accuracy of the tagger against the gold standard.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m    Strip the tags from the gold standard text, retag it using\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m    :rtype: float\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     tagged_sents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtag_sents(untag(sent) \u001b[39mfor\u001b[39;49;00m sent \u001b[39min\u001b[39;49;00m gold)\n\u001b[0;32m     75\u001b[0m     gold_tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chain\u001b[39m.\u001b[39mfrom_iterable(gold))\n\u001b[0;32m     76\u001b[0m     test_tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chain\u001b[39m.\u001b[39mfrom_iterable(tagged_sents))\n",
      "File \u001b[1;32mc:\\Users\\sanfe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tag\\api.py:57\u001b[0m, in \u001b[0;36mTaggerI.tag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtag_sents\u001b[39m(\u001b[39mself\u001b[39m, sentences):\n\u001b[0;32m     52\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    Apply ``self.tag()`` to each element of *sentences*.  I.e.::\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[39m        return [self.tag(sent) for sent in sentences]\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtag(sent) \u001b[39mfor\u001b[39;49;00m sent \u001b[39min\u001b[39;49;00m sentences]\n",
      "File \u001b[1;32mc:\\Users\\sanfe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tag\\api.py:57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtag_sents\u001b[39m(\u001b[39mself\u001b[39m, sentences):\n\u001b[0;32m     52\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    Apply ``self.tag()`` to each element of *sentences*.  I.e.::\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[39m        return [self.tag(sent) for sent in sentences]\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtag(sent) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences]\n",
      "\u001b[1;32mc:\\Users\\sanfe\\OneDrive\\Desktop\\NLP\\NLP\\NLP-Projects\\alesssa_in4080_mandatory_2.ipynb Cell 43\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m history \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sentence):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     featureset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(sentence, i, history)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     test_features\u001b[39m.\u001b[39mappend(featureset)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m X_test \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorizer\u001b[39m.\u001b[39mtransform(test_features)\n",
      "\u001b[1;32mc:\\Users\\sanfe\\OneDrive\\Desktop\\NLP\\NLP\\NLP-Projects\\alesssa_in4080_mandatory_2.ipynb Cell 43\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     features[\u001b[39m\"\u001b[39m\u001b[39mprev_word\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m sentence[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     features[\u001b[39m\"\u001b[39m\u001b[39mprev_tag\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m history[i\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y143sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def pos_features_tag(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "        features[\"prev_tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    return features\n",
    "\n",
    "lr_tagger = ScikitGreedyTagger(pos_features_tag)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanfe\\OneDrive\\Desktop\\NLP\\NLP\\NLP-Projects\\alesssa_in4080_mandatory_2.ipynb Cell 44\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m lr_tagger\u001b[39m.\u001b[39mtrain(news_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(lr_tagger\u001b[39m.\u001b[39;49maccuracy(news_val))\n",
      "File \u001b[1;32mc:\\Users\\sanfe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tag\\api.py:74\u001b[0m, in \u001b[0;36mTaggerI.accuracy\u001b[1;34m(self, gold)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccuracy\u001b[39m(\u001b[39mself\u001b[39m, gold):\n\u001b[0;32m     64\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m    Score the accuracy of the tagger against the gold standard.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m    Strip the tags from the gold standard text, retag it using\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m    :rtype: float\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     tagged_sents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtag_sents(untag(sent) \u001b[39mfor\u001b[39;49;00m sent \u001b[39min\u001b[39;49;00m gold)\n\u001b[0;32m     75\u001b[0m     gold_tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chain\u001b[39m.\u001b[39mfrom_iterable(gold))\n\u001b[0;32m     76\u001b[0m     test_tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chain\u001b[39m.\u001b[39mfrom_iterable(tagged_sents))\n",
      "File \u001b[1;32mc:\\Users\\sanfe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tag\\api.py:57\u001b[0m, in \u001b[0;36mTaggerI.tag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtag_sents\u001b[39m(\u001b[39mself\u001b[39m, sentences):\n\u001b[0;32m     52\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    Apply ``self.tag()`` to each element of *sentences*.  I.e.::\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[39m        return [self.tag(sent) for sent in sentences]\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtag(sent) \u001b[39mfor\u001b[39;49;00m sent \u001b[39min\u001b[39;49;00m sentences]\n",
      "File \u001b[1;32mc:\\Users\\sanfe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tag\\api.py:57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtag_sents\u001b[39m(\u001b[39mself\u001b[39m, sentences):\n\u001b[0;32m     52\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    Apply ``self.tag()`` to each element of *sentences*.  I.e.::\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[39m        return [self.tag(sent) for sent in sentences]\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtag(sent) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences]\n",
      "\u001b[1;32mc:\\Users\\sanfe\\OneDrive\\Desktop\\NLP\\NLP\\NLP-Projects\\alesssa_in4080_mandatory_2.ipynb Cell 44\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m history \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sentence):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     featureset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(sentence, i, history)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     test_features\u001b[39m.\u001b[39mappend(featureset)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m X_test \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorizer\u001b[39m.\u001b[39mtransform(test_features)\n",
      "\u001b[1;32mc:\\Users\\sanfe\\OneDrive\\Desktop\\NLP\\NLP\\NLP-Projects\\alesssa_in4080_mandatory_2.ipynb Cell 44\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39melif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     features[\u001b[39m\"\u001b[39m\u001b[39mprev_word\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m sentence[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     features[\u001b[39m\"\u001b[39m\u001b[39mprev_tag\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m history[i\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     features[\u001b[39m\"\u001b[39m\u001b[39m2_prev_tag\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<START>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/OneDrive/Desktop/NLP/NLP/NLP-Projects/alesssa_in4080_mandatory_2.ipynb#Y166sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def pos_features_trigram(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "        features[\"prev_tag\"] = \"<START>\"\n",
    "        features[\"2_prev_tag\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "        features[\"2_prev_tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "        features[\"2_prev_tag\"] = history[i-2]\n",
    "    return features\n",
    "\n",
    "lr_tagger = ScikitGreedyTagger(pos_features_trigram)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2d:\tEven\tmore\tfeatures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to add more features to get an even better tagger. Only the fantasy sets limits to what you may \n",
    "consider. Some ideas: Extract suffixes and prefixes from the current, previous or next word. Is the \n",
    "current word a number? Is it capitalized? Does it contain capitals? Does it contain a hyphen? etc. What is \n",
    "the best feature set you can come up with? Train and test various feature sets and select the best one.\n",
    "If you use sources for finding tips about good features (like articles, web pages, NLTK code, etc.) make \n",
    "references to the sources and explain what you got from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2e:\tRegularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment, we will study the effect of different regularization strengths now. In scikit\u0002learn, regularization is expressed by the parameter C. A smaller C means stronger regularization. Try with \n",
    "C in [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] and see which value which yields the best result. You can also try \n",
    "additional values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "#create a for cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize your experiments to make clear which set of features and parameters provide the best \n",
    "results, and what the corresponding accuracy score is. Did you manage to outperform the perceptron \n",
    "tagger? If not, where do you think the bottleneck of your current tagger lies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part\t3\t– Training\tand\ttesting\ton\ta\tlarger\tcorpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus covers 15 different genres, but we have only explored the news genre so far. In this \n",
    "part, we will retrain the most promising taggers on an extended set of genres and test them on held-out \n",
    "data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t3a:\tCompile\tthe\textended\ttraining\tand\ttest\tdata**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK book, chapter 2.1.3, lists the names of the 15 genres available in the Brown corpus. We will set \n",
    "two genres aside for testing: hobbies and adventure. For training, we will use the news training set \n",
    "prepared for the previous exercises, as well as the data from the remaining 12 genres. Prepare the \n",
    "corpus as described and store the datasets in the variables all_train, hobbies_test and \n",
    "adventure_test. We will not use news_val in this part. Make sure to use the universal tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = brown.tagged_sents(categories = ['news', 'editorial', 'reviews', 'religion', 'lore', 'belles_lettres', \n",
    "                                             'government', 'crime', 'science_fiction', 'romance', 'humor', 'learned', 'mystery'], \n",
    "                                             tagset = 'universal')\n",
    "hobbies_test = brown.tagged_sents(categories = 'hobbies', tagset = 'universal')\n",
    "adventure_test = brown.tagged_sents(categories = 'adventure', tagset = 'universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t3b:\tEvaluate\tthe\ttaggers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the most successful tagger from part 1 and the best setup from part 2. Retrain both of them on \n",
    "all_train and evaluate them separately on the two test genres. Report the results and discuss them \n",
    "briefly: Which of the two genres is “easier”? How well do the two taggers generalize to unseen genres?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy hobbies:\n",
      "0.9676240208877285\n",
      "Accuracy adventures:\n",
      "0.9735081191774105\n"
     ]
    }
   ],
   "source": [
    "perc = nltk.PerceptronTagger(load = False)\n",
    "perc.train(all_train)\n",
    "perc_accuracy_h = perc.accuracy(hobbies_test)\n",
    "print(\"Accuracy hobbies:\")\n",
    "print(perc_accuracy_h)\n",
    "perc_accuracy_a = perc.accuracy(adventure_test)\n",
    "print(\"Accuracy adventures:\")\n",
    "print(perc_accuracy_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point 2 best???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t3c:\tConfusion\tmatrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy gives us a high-level overview of the performance of a tagger, but we may be interested in \n",
    "finding out more details about where the tagger makes the mistakes. The universal tagset is reasonably \n",
    "small, so we can produce a confusion matrix. Take a look at https://www.nltk.org/api/nltk.tag.api.html\n",
    "and make a confusion matrix for the results. Pick the results of one test set and one tagger. Make sure \n",
    "you understand what the rows and columns are. Which pairs of tags are most easily confounded?\n",
    "You can find the documentation of the tagset in the following link, but note that NLTK uses an earlier, \n",
    "slightly different version of the tagset: https://universaldependencies.org/u/pos/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |                             C           N           P           V       |\n",
      "     |           A     A     A     O     D     O     N     R     P     E       |\n",
      "     |           D     D     D     N     E     U     U     O     R     R       |\n",
      "     |     .     J     P     V     J     T     N     M     N     T     B     X |\n",
      "-----+-------------------------------------------------------------------------+\n",
      "   . |<10929>    .     .     .     .     .     .     .     .     .     .     . |\n",
      " ADJ |     . <3018>    5   120     .     .   156     1     .     1    63     . |\n",
      " ADP |     .    12 <6898>   37     3     .    11     .     3    99     6     . |\n",
      " ADV |     .   110    55 <3593>    9    15    64     .     .    25     8     . |\n",
      "CONJ |     .     .     5     . <2153>   12     1     .     .     .     2     . |\n",
      " DET |     .     .    20    13     2 <8076>    3     2    39     .     .     . |\n",
      "NOUN |     .   167     2    38     .     .<13009>   16     1     8   113     . |\n",
      " NUM |     .     1     .     .     .     .     3  <462>    .     .     .     . |\n",
      "PRON |     .     .    10     1     .    88    11     . <5094>    .     1     . |\n",
      " PRT |     .     5   135    16     .     .    70     .     1 <2199>   10     . |\n",
      "VERB |     .    26    11    12     .     .   146     1     .     5<12073>    . |\n",
      "   X |     .     .     .     .     .     .    35     .     .     1     1    <1>|\n",
      "-----+-------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(perc.confusion(adventure_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point 2 best???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t3d:\tPrecision,\trecall\tand\tf-measure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding hints on the NLTK web page linked above, calculate the precision, recall and f-measure for each \n",
    "tag and display the results in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tag | Prec.  | Recall | F-measure\n",
      "-----+--------+--------+-----------\n",
      "   . | 1.0000 | 1.0000 | 1.0000\n",
      " ADJ | 0.9039 | 0.8971 | 0.9005\n",
      " ADP | 0.9660 | 0.9758 | 0.9709\n",
      " ADV | 0.9381 | 0.9263 | 0.9322\n",
      "CONJ | 0.9935 | 0.9908 | 0.9922\n",
      " DET | 0.9860 | 0.9903 | 0.9881\n",
      "NOUN | 0.9630 | 0.9742 | 0.9685\n",
      " NUM | 0.9585 | 0.9914 | 0.9747\n",
      "PRON | 0.9914 | 0.9787 | 0.9850\n",
      " PRT | 0.9405 | 0.9027 | 0.9212\n",
      "VERB | 0.9834 | 0.9836 | 0.9835\n",
      "   X | 1.0000 | 0.0263 | 0.0513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(perc.evaluate_per_tag(adventure_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also calculate the macro precision, macro recall and macro f-measure across all tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t3e:\tError\tanalysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it makes sense to inspect the output of a machine learning model more thoroughly. Find five \n",
    "sentences in the test set where at least one token is misclassified and display these sentences in the \n",
    "following format, with both the predicted and gold tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = brown.tagged_words(categories='adventure', tagset='universal')\n",
    "sentences = brown.sents(categories='adventure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 :\n",
      "Word: He\tPredicted Tag: PRON\tGold Tag: NOUN\n",
      "Word: was\tPredicted Tag: VERB\tGold Tag: NOUN\n",
      "Word: well\tPredicted Tag: ADV\tGold Tag: VERB\n",
      "Word: rid\tPredicted Tag: ADJ\tGold Tag: PRON\n",
      "Word: of\tPredicted Tag: ADP\tGold Tag: PRON\n",
      "Word: her\tPredicted Tag: PRON\tGold Tag: VERB\n",
      "Word: .\tPredicted Tag: .\tGold Tag: VERB\n",
      "\n",
      "\n",
      "Sentence 2 :\n",
      "Word: He\tPredicted Tag: PRON\tGold Tag: NOUN\n",
      "Word: certainly\tPredicted Tag: ADV\tGold Tag: NOUN\n",
      "Word: didn't\tPredicted Tag: VERB\tGold Tag: VERB\n",
      "Word: want\tPredicted Tag: VERB\tGold Tag: PRON\n",
      "Word: a\tPredicted Tag: DET\tGold Tag: PRON\n",
      "Word: wife\tPredicted Tag: NOUN\tGold Tag: VERB\n",
      "Word: who\tPredicted Tag: PRON\tGold Tag: VERB\n",
      "Word: was\tPredicted Tag: VERB\tGold Tag: NOUN\n",
      "Word: fickle\tPredicted Tag: NOUN\tGold Tag: NOUN\n",
      "Word: as\tPredicted Tag: ADP\tGold Tag: .\n",
      "Word: Ann\tPredicted Tag: NOUN\tGold Tag: PRON\n",
      "Word: .\tPredicted Tag: .\tGold Tag: VERB\n",
      "\n",
      "\n",
      "Sentence 3 :\n",
      "Word: If\tPredicted Tag: ADP\tGold Tag: NOUN\n",
      "Word: he\tPredicted Tag: PRON\tGold Tag: NOUN\n",
      "Word: had\tPredicted Tag: VERB\tGold Tag: VERB\n",
      "Word: married\tPredicted Tag: VERB\tGold Tag: PRON\n",
      "Word: her\tPredicted Tag: PRON\tGold Tag: PRON\n",
      "Word: ,\tPredicted Tag: .\tGold Tag: VERB\n",
      "Word: he'd\tPredicted Tag: PRT\tGold Tag: VERB\n",
      "Word: have\tPredicted Tag: VERB\tGold Tag: NOUN\n",
      "Word: been\tPredicted Tag: VERB\tGold Tag: NOUN\n",
      "Word: asking\tPredicted Tag: VERB\tGold Tag: .\n",
      "Word: for\tPredicted Tag: ADP\tGold Tag: PRON\n",
      "Word: trouble\tPredicted Tag: NOUN\tGold Tag: VERB\n",
      "Word: .\tPredicted Tag: .\tGold Tag: ADV\n",
      "\n",
      "\n",
      "Sentence 4 :\n",
      "Word: But\tPredicted Tag: CONJ\tGold Tag: NOUN\n",
      "Word: all\tPredicted Tag: PRT\tGold Tag: NOUN\n",
      "Word: of\tPredicted Tag: ADP\tGold Tag: VERB\n",
      "Word: this\tPredicted Tag: DET\tGold Tag: PRON\n",
      "Word: was\tPredicted Tag: VERB\tGold Tag: PRON\n",
      "Word: rationalization\tPredicted Tag: NOUN\tGold Tag: VERB\n",
      "Word: .\tPredicted Tag: .\tGold Tag: VERB\n",
      "\n",
      "\n",
      "Sentence 5 :\n",
      "Word: Sometimes\tPredicted Tag: ADV\tGold Tag: NOUN\n",
      "Word: he\tPredicted Tag: PRON\tGold Tag: NOUN\n",
      "Word: woke\tPredicted Tag: VERB\tGold Tag: VERB\n",
      "Word: up\tPredicted Tag: PRT\tGold Tag: PRON\n",
      "Word: in\tPredicted Tag: ADP\tGold Tag: PRON\n",
      "Word: the\tPredicted Tag: DET\tGold Tag: VERB\n",
      "Word: middle\tPredicted Tag: NOUN\tGold Tag: VERB\n",
      "Word: of\tPredicted Tag: ADP\tGold Tag: NOUN\n",
      "Word: the\tPredicted Tag: DET\tGold Tag: NOUN\n",
      "Word: night\tPredicted Tag: NOUN\tGold Tag: .\n",
      "Word: thinking\tPredicted Tag: NOUN\tGold Tag: PRON\n",
      "Word: of\tPredicted Tag: ADP\tGold Tag: VERB\n",
      "Word: Ann\tPredicted Tag: NOUN\tGold Tag: ADV\n",
      "Word: ,\tPredicted Tag: .\tGold Tag: ADJ\n",
      "Word: and\tPredicted Tag: CONJ\tGold Tag: ADP\n",
      "Word: then\tPredicted Tag: ADV\tGold Tag: PRON\n",
      "Word: could\tPredicted Tag: VERB\tGold Tag: .\n",
      "Word: not\tPredicted Tag: ADV\tGold Tag: PRON\n",
      "Word: get\tPredicted Tag: VERB\tGold Tag: ADV\n",
      "Word: back\tPredicted Tag: ADV\tGold Tag: VERB\n",
      "Word: to\tPredicted Tag: ADP\tGold Tag: VERB\n",
      "Word: sleep\tPredicted Tag: NOUN\tGold Tag: DET\n",
      "Word: .\tPredicted Tag: .\tGold Tag: NOUN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a counter for misclassified sentences\n",
    "misclassified_count = 0\n",
    "i = 1\n",
    "\n",
    "for sentence, tag in zip(sentences, tagged_words):\n",
    "    predicted = [tag for word, tag in perc.tag(sentence)]\n",
    "    if any (predicted_tag != gold_tag[1] for predicted_tag, gold_tag in zip(predicted, tagged_words)):\n",
    "        print(\"Sentence\", i, \":\")\n",
    "        for word, predicted_tag, gold_tag in zip(sentence, predicted, tagged_words):\n",
    "            print(f\"Word: {word}\\tPredicted Tag: {predicted_tag}\\tGold Tag: {gold_tag[1]}\")\n",
    "        print(\"\\n\")\n",
    "        misclassified_count += 1\n",
    "        i += 1\n",
    "    \n",
    "    # Stop after finding five misclassified sentences\n",
    "    if misclassified_count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified Sentence 1:\n",
      "  Token Predicted Tag Gold Tag\n",
      "0    He          PRON     NOUN\n",
      "1   was          VERB     NOUN\n",
      "2  well           ADV     VERB\n",
      "3   rid           ADJ     PRON\n",
      "4    of           ADP     PRON\n",
      "5   her          PRON     VERB\n",
      "6     .             .     VERB\n",
      "\n",
      "\n",
      "Misclassified Sentence 2:\n",
      "        Token Predicted Tag Gold Tag\n",
      "0          He          PRON     NOUN\n",
      "1   certainly           ADV     NOUN\n",
      "2      didn't          VERB     VERB\n",
      "3        want          VERB     PRON\n",
      "4           a           DET     PRON\n",
      "5        wife          NOUN     VERB\n",
      "6         who          PRON     VERB\n",
      "7         was          VERB     NOUN\n",
      "8      fickle          NOUN     NOUN\n",
      "9          as           ADP        .\n",
      "10        Ann          NOUN     PRON\n",
      "11          .             .     VERB\n",
      "\n",
      "\n",
      "Misclassified Sentence 3:\n",
      "      Token Predicted Tag Gold Tag\n",
      "0        If           ADP     NOUN\n",
      "1        he          PRON     NOUN\n",
      "2       had          VERB     VERB\n",
      "3   married          VERB     PRON\n",
      "4       her          PRON     PRON\n",
      "5         ,             .     VERB\n",
      "6      he'd           PRT     VERB\n",
      "7      have          VERB     NOUN\n",
      "8      been          VERB     NOUN\n",
      "9    asking          VERB        .\n",
      "10      for           ADP     PRON\n",
      "11  trouble          NOUN     VERB\n",
      "12        .             .      ADV\n",
      "\n",
      "\n",
      "Misclassified Sentence 4:\n",
      "             Token Predicted Tag Gold Tag\n",
      "0              But          CONJ     NOUN\n",
      "1              all           PRT     NOUN\n",
      "2               of           ADP     VERB\n",
      "3             this           DET     PRON\n",
      "4              was          VERB     PRON\n",
      "5  rationalization          NOUN     VERB\n",
      "6                .             .     VERB\n",
      "\n",
      "\n",
      "Misclassified Sentence 5:\n",
      "        Token Predicted Tag Gold Tag\n",
      "0   Sometimes           ADV     NOUN\n",
      "1          he          PRON     NOUN\n",
      "2        woke          VERB     VERB\n",
      "3          up           PRT     PRON\n",
      "4          in           ADP     PRON\n",
      "5         the           DET     VERB\n",
      "6      middle          NOUN     VERB\n",
      "7          of           ADP     NOUN\n",
      "8         the           DET     NOUN\n",
      "9       night          NOUN        .\n",
      "10   thinking          NOUN     PRON\n",
      "11         of           ADP     VERB\n",
      "12        Ann          NOUN      ADV\n",
      "13          ,             .      ADJ\n",
      "14        and          CONJ      ADP\n",
      "15       then           ADV     PRON\n",
      "16      could          VERB        .\n",
      "17        not           ADV     PRON\n",
      "18        get          VERB      ADV\n",
      "19       back           ADV     VERB\n",
      "20         to           ADP     VERB\n",
      "21      sleep          NOUN      DET\n",
      "22          .             .     NOUN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "misclassified_count = 0\n",
    "\n",
    "# Initialize a list to store misclassified tokens\n",
    "\n",
    "misclassified_dfs = []\n",
    "\n",
    "for sentence, gold_tag in zip(sentences, tagged_words):\n",
    "    predicted = [tag for word, tag in perc.tag(sentence)]\n",
    "    if any(predicted_tag != gold_tag for predicted_tag, (_, gold_tag) in zip(predicted, tagged_words)):\n",
    "        misclassified_tokens = []\n",
    "        for word, predicted_tag, (_, gold_tag) in zip(sentence, predicted, tagged_words):\n",
    "            #if predicted_tag != gold_tag:\n",
    "                misclassified_tokens.append([word, predicted_tag, gold_tag])\n",
    "\n",
    "        # Create a DataFrame from the list of misclassified tokens\n",
    "        df = pd.DataFrame(misclassified_tokens, columns = [\"Token\", \"Predicted Tag\", \"Gold Tag\"])\n",
    "        misclassified_dfs.append(df)\n",
    "        misclassified_count += 1\n",
    "        misclassified_tokens = []\n",
    "    # Stop after finding five misclassified sentences\n",
    "    if misclassified_count == 5:\n",
    "        break\n",
    "\n",
    "# Access and print each DataFrame in the list\n",
    "for i in range(len(misclassified_dfs)):\n",
    "    print(f\"Misclassified Sentence {i+1}:\")\n",
    "    print(misclassified_dfs[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the words that are tagged differently. Comment on each of the differences. Would you say that \n",
    "the predicted tag is wrong? Or is there a genuine ambiguity such that both answers are defendable? Or is \n",
    "even the gold tag wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
