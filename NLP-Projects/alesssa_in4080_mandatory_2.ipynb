{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IN4080 - 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mandatory Assignment 2** - Alessia Sanfelici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1\t– Exploring\tthe NLTK\ttagger landscape**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1a:\tData\tSplit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first experiments, we limit ourselves to the news section of the Brown corpus and split it into a \n",
    "training (90%) and a validation (10%) set. (We don’t need a test set for the moment, but will build one in \n",
    "part 3.) Moreover, we use the universal tagset instead of the default one.\n",
    "\n",
    "You should store your data split in the variables news_train and news_val. Note that you may need \n",
    "to download the corpus first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\sanfe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = brown.tagged_sents(categories = 'news', tagset = 'universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train, news_val = train_test_split(sents, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1b:\tMost\tFrequent\tClass\tBaseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of part-of-speech tags is typically quite skewed, with the most frequent class in general \n",
    "being common nouns. As a simple baseline, we should thus know how a model that always predicts the \n",
    "same (most frequent) class performs. This can be done with nltk.DefaultTagger. Note that we are using the universal tagset, so the \n",
    "most frequent tag is not named NN. Evaluate it on the validation set and report the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent tag is called NOUN\n"
     ]
    }
   ],
   "source": [
    "tags = [tag for (word, tag) in sents[0]]\n",
    "max_tag = nltk.FreqDist(tags).max()\n",
    "print(\"The most frequent tag is called\", max_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.2996151189183855\n"
     ]
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "default_accuracy = default_tagger.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(default_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1c:\tNaïve\tBayes\tUnigram\tTagger**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first models discussed in course is a Naïve Bayes classifier that relies only on the current word \n",
    "and does not take any context into account. This model is available as nltk.UnigramTagger. Report the accuracy on the \n",
    "validation set. How does the accuracy on the universal tagset differ from the one reported on the default \n",
    "tagset in the NLTK book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.8853251751702359\n"
     ]
    }
   ],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(news_train)\n",
    "unigram_accuracy = unigram_tagger.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(unigram_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy in this case results be be lower than the one reported on the default tagset in the NLTK book. It is lower of around 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1d:\tBigram\tHMM\tTagger**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lectures, we spent quite some time on the HMM tagger. Evaluate it on the validation set and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9121681634264285\n"
     ]
    }
   ],
   "source": [
    "hmm = nltk.HiddenMarkovModelTagger.train(news_train)\n",
    "hmm_accuracy = hmm.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(hmm_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t1e:\tPerceptron\twith\tgreedy\tdecoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lectures, we have shortly discussed Matthew Honnibal’s proposal of a structured perceptron\n",
    "tagger with greedy decoding. He argued that an extended set of features is more helpful for tagging than \n",
    "exact (Viterbi) decoding. Evaluate it on the validation set and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9654593901115168\n"
     ]
    }
   ],
   "source": [
    "perc = nltk.PerceptronTagger(load = False)\n",
    "perc.train(news_train)\n",
    "perc_accuracy = perc.accuracy(news_val)\n",
    "print(\"Accuracy:\")\n",
    "print(perc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the results of the previous exercises and discuss them in a few sentences. Do the accuracies \n",
    "correspond to your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Default Tagger</th>\n",
       "      <td>0.299615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigram Tagger</th>\n",
       "      <td>0.885325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Markov Model Tagger</th>\n",
       "      <td>0.912168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron Tagger</th>\n",
       "      <td>0.965459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy\n",
       "Default Tagger              0.299615\n",
       "Unigram Tagger              0.885325\n",
       "Hidden Markov Model Tagger  0.912168\n",
       "Perceptron Tagger           0.965459"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = [default_accuracy, unigram_accuracy, hmm_accuracy, perc_accuracy]\n",
    "taggers = [\"Default Tagger\", \"Unigram Tagger\", \"Hidden Markov Model Tagger\", \"Perceptron Tagger\"]\n",
    "\n",
    "df_accuracies = pd.DataFrame(accuracies, columns = [\"Accuracy\"], index = taggers)\n",
    "df_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... discussion ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part\t2\t– Greedy\tLR\ttaggers\tand\tfeature\tengineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2a:\tGetting\tstarted\twith\ta\tgreedy\tlogistic\tregression tagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScikitGreedyTagger(nltk.TaggerI):\n",
    "    def __init__(self, features, clf=LogisticRegression(max_iter = 400)):\n",
    "        self.features = features\n",
    "        self.classifier = clf\n",
    "        self.vectorizer = DictVectorizer()\n",
    "\n",
    "    def train(self, train_sents):\n",
    "        train_feature_sets = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                feature_set = self.features(untagged_sent, i, history)\n",
    "                train_feature_sets.append(feature_set)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        x_train = self.vectorizer.fit_transform(train_feature_sets)\n",
    "        y_train = np.array(train_labels)\n",
    "        self.classifier.fit(x_train, y_train)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.vectorizer.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.924010658245337\n"
     ]
    }
   ],
   "source": [
    "lr_tagger = ScikitGreedyTagger(pos_features)\n",
    "lr_tagger.train(news_train)\n",
    "print(\"Accuracy:\")\n",
    "print(lr_tagger.accuracy(news_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the accuracy of this tagger compare to the taggers tested in part 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".........."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise\t2b:\tAdding\tword\tcontext\tfeatures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic feature function contains the previous and the current word. Also add the next word and the \n",
    "word before the previous one. Describe which combination works best and keep it for the next \n",
    "experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "        features[\"2_prev_word\"] = \"<START>\"\n",
    "        features[\"next_word\"] = sentence[i+1]\n",
    "    elif i == 1:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"2_prev_word\"] = \"<START>\"\n",
    "        features[\"next_word\"] = sentence[i+1]\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"2_prev_word\"] = sentence[i-2]\n",
    "        features[\"next_word\"] = sentence[i+1]\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the part for the last word!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
